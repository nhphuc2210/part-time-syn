{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "88adc4a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "import sys,os,shutil\n",
    "sys.path.append(\"..\")\n",
    "import argparse\n",
    "import logging\n",
    "from office365.runtime.auth.user_credential import UserCredential\n",
    "from office365.sharepoint.client_context import ClientContext\n",
    "\n",
    "from snowflake import connector\n",
    "from os import getenv\n",
    "from collections import namedtuple\n",
    "\n",
    "from datetime import datetime\n",
    "from openpyxl import load_workbook\n",
    "from collections import Counter\n",
    "\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "# from common.util.database.snowflake import SnowflakeLoader\n",
    "# from common.loader.excel_loader.util import source_to_parquet\n",
    "# from common.constant.mode import ETLMode\n",
    "# license_hp_service = 'C:/cloud_service'\n",
    "# sys.path.append(license_hp_service)\n",
    "# import hoangphuc__function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5e48a8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_from_sharepoint(account_sharepoint, pwsd_sharepoint, site_url, file_name, file_path, download_to):\n",
    "    try:\n",
    "        # credentials = ClientCredential(account_sharepoint, pwsd_sharepoint)\n",
    "        # ctx = ClientContext(site_url).with_credentials(credentials)\n",
    "\n",
    "        ctx = ClientContext(site_url).with_credentials(UserCredential(account_sharepoint, pwsd_sharepoint))\n",
    "        _file = open(download_to, \"wb\")\n",
    "        ctx.web.get_file_by_server_relative_path(file_path).download(\n",
    "                _file\n",
    "            ).execute_query()\n",
    "        print(f\"====Downloaded Source File for {file_name}====\")\n",
    "        _file.close()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        input(\"Cannot download. Press any key to exit.\")\n",
    "\n",
    "def create_folder_if_not_exists(folder):\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    " \n",
    "def del_file_in_folder(folder):\n",
    "    for filename in os.listdir(folder):\n",
    "        file_path = os.path.join(folder, filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)\n",
    "        except Exception as e:\n",
    "            print('Failed to delete %s. Reason: %s' % (file_path, e))\n",
    "\n",
    "def ask__yes_no(mess):\n",
    "    while True:\n",
    "        answer = input('\\n\\t{} y/n: '.format(mess))\n",
    "        if answer in ('y','n'):\n",
    "            break\n",
    "        else:\n",
    "            print(\"\\tInvalid input\")\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "df31c0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_folder__cdm = 'c:/cdm/'\n",
    "create_folder_if_not_exists(report_folder__cdm)\n",
    "del_file_in_folder(report_folder__cdm)\n",
    " \n",
    "import_snowflake = 'C:/import_snowflake/'\n",
    "create_folder_if_not_exists(import_snowflake)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ef8143a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_lists = [\n",
    "\"/sites/BAS/Shared Documents/4.CDM/data-requirements/cdm__quarterly_data.csv\",\n",
    "\"/sites/BAS/Shared Documents/4.CDM/data-requirements/cdm__seller_code_mapping.csv\",\n",
    "\"/sites/BAS/Shared Documents/4.CDM/data-requirements/cdm__weekly_data.csv\",\n",
    "\"/sites/BAS/Shared Documents/4.CDM/data-requirements/certificate_target.csv\",\n",
    "\"/sites/BAS/Shared Documents/4.CDM/data-requirements/point_target.csv\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d8cad8bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====your account = nhoang@synagie.com\n",
      "====your password = Ibethere4u@188\n",
      "\n",
      "\n",
      "cdm__quarterly_data.csv is downloading...\n",
      "====Downloaded Source File for cdm__quarterly_data.csv====\n",
      "\n",
      "cdm__seller_code_mapping.csv is downloading...\n",
      "====Downloaded Source File for cdm__seller_code_mapping.csv====\n",
      "\n",
      "cdm__weekly_data.csv is downloading...\n",
      "====Downloaded Source File for cdm__weekly_data.csv====\n",
      "\n",
      "certificate_target.csv is downloading...\n",
      "====Downloaded Source File for certificate_target.csv====\n",
      "\n",
      "point_target.csv is downloading...\n",
      "====Downloaded Source File for point_target.csv====\n"
     ]
    }
   ],
   "source": [
    "site_url = \"https://avenzacorp.sharepoint.com/sites/BAS/\"\n",
    "account_sharepoint = getenv('account_sharepoint')\n",
    "pwsd_sharepoint = getenv('pwsd_sharepoint')\n",
    "\n",
    "print(f\"\"\"\n",
    "====your account = {account_sharepoint}\n",
    "====your password = {pwsd_sharepoint}\n",
    "\"\"\")\n",
    "# account_sharepoint = input(\"Pls input your company account ex. phucnguyen@synagie.com: \")\n",
    "# pwsd_sharepoint = input(\"Pls input your password: \")\n",
    "\n",
    "download_folder = report_folder__cdm\n",
    "\n",
    "answer = ask__yes_no(\"Do you want do re-download from sharepoint? \")\n",
    "if answer == 'y':\n",
    "    create_folder_if_not_exists(download_folder)    \n",
    "    del_file_in_folder(download_folder)\n",
    " \n",
    "    for file_path in raw_lists:\n",
    "        file_name = file_path.split(\"/\")[-1] \n",
    "        print(f'\\n{file_name} is downloading...')\n",
    "\n",
    "        download_to = os.path.join(download_folder, file_name)\n",
    "        download_from_sharepoint(account_sharepoint, pwsd_sharepoint, site_url, file_name, file_path, download_to)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b23198af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SnowflakeLoader:\n",
    "   def __init__(self, schema, target_table):\n",
    "       self.schema = schema.upper()\n",
    "       self.cred = self.get_cred()\n",
    "       self.cred[\"schema\"] = self.schema\n",
    "       self.cur = self.connect(self.cred)\n",
    "       self.target_table = target_table.upper()\n",
    "       self.target_stg_table = f\"{target_table}_STG\"\n",
    "\n",
    "   def get_cred(self):\n",
    "       return {\n",
    "           \"user\": getenv(\"dwh_username\",\"input\" ),\n",
    "           \"password\": getenv(\"dwh_password\", \"input\"),\n",
    "           \"account\": getenv(\"dwh_account\", \"input\"),\n",
    "           \"role\": getenv(\"dwh_role\",'X_BAS_BI_USER'),\n",
    "           \"database\": getenv(\"dwh_database\", \"DWH\"),\n",
    "           \"warehouse\":getenv('warehouse','COMPUTE_WH')\n",
    "       }\n",
    "\n",
    "   def connect(self, cred):\n",
    "       self.conn = connector.connect(**cred)\n",
    "       return self.conn.cursor()\n",
    "\n",
    "   def get_columns(self, tablename):\n",
    "       result = self.cur.execute(\n",
    "           f\"\"\"SELECT\n",
    "       COLUMN_NAME , DATA_TYPE\n",
    "       FROM INFORMATION_SCHEMA.COLUMNS\n",
    "       WHERE TABLE_SCHEMA = '{self.schema}'\n",
    "       AND TABLE_NAME = '{tablename}'\"\"\"\n",
    "       ).fetchall()\n",
    "\n",
    "       return {x[0]: x[1] for x in result}\n",
    "\n",
    "   def create_table(self, col_definition, table=None):\n",
    "\n",
    "       if table is None:\n",
    "           table = self.target_table\n",
    "\n",
    "       return self.cur.execute(\n",
    "           f\"CREATE TABLE IF NOT EXISTS {table} ({col_definition});\"\n",
    "       ).fetchone()\n",
    "\n",
    "   def add_column(self, colname, datatype):\n",
    "       return self.cur.execute(\n",
    "           f\"ALTER TABLE {self.target_table} ADD {colname} {datatype}\"\n",
    "       ).fetchone()\n",
    "\n",
    "   def put_file(self, path):\n",
    "       return self.cur.execute(\n",
    "           f\"put file://{path} @{self.target_stg_table};\"\n",
    "       ).fetchone()\n",
    "\n",
    "   def create_stg_table(self):\n",
    "       return self.cur.execute(\n",
    "           f\"CREATE OR REPLACE STAGE {self.target_stg_table} file_format = (type = 'PARQUET');\"\n",
    "       ).fetchone()\n",
    "\n",
    "   def close(self):\n",
    "       self.cur.close()\n",
    "       self.conn.close()\n",
    "\n",
    "   def drop_table(self, table):\n",
    "       return self.cur.execute(f\"DROP TABLE IF EXISTS {table}\").fetchone()\n",
    "\n",
    "   def table_def_update(self):\n",
    "       target_cols = self.get_columns(self.schema, self.target_table)\n",
    "       stg_cols = self.get_columns(self.schema, self.target_stg_table)\n",
    "\n",
    "       new_cols = set(target_cols.keys()) - set(stg_cols.keys())\n",
    "       for new_col in new_cols:\n",
    "           self.add_column(new_col, target_cols[new_col])\n",
    "\n",
    "   def overwrite_target_table(self, col_definition, cols):\n",
    "\n",
    "       swap_table = f\"swap_{self.target_table}\"\n",
    "       temp_table = f\"temp_{self.target_table}\"\n",
    "\n",
    "       self.drop_table(swap_table)\n",
    "       self.create_table(col_definition, swap_table)\n",
    "       self.copy_from_stage(cols, swap_table)\n",
    "       self.drop_table(temp_table)\n",
    "       self.swap_table(self.target_table, swap_table, temp_table)\n",
    "\n",
    "   def swap_table(self, target: str, source: str, temp: str) -> str:\n",
    "       result = self.cur.execute(f\"ALTER TABLE {target} RENAME TO {temp};\").fetchone()\n",
    "       result1 = self.cur.execute(\n",
    "           f\"ALTER TABLE {source} RENAME TO {target};\"\n",
    "       ).fetchone()\n",
    "       return {result, result1}\n",
    "\n",
    "   def copy_from_stage(self, cols, tablename):\n",
    "\n",
    "       statement = \"\"\n",
    "       for col in cols:\n",
    "           if col == \"DWH_TIMESTAMP\":\n",
    "               statement += f'to_timestamp($1:\"DWH_TIMESTAMP\"::int, 9),'\n",
    "           else:\n",
    "               statement += f'$1:\"{col}\",'\n",
    "\n",
    "       return self.cur.execute(\n",
    "           f\"\"\"COPY INTO {tablename} FROM (SELECT {statement[:-1]} FROM @{self.target_stg_table})\"\"\"\n",
    "       ).fetchone()\n",
    "\n",
    "   def get_count(self, table=None) -> int:\n",
    "\n",
    "       if table is None:\n",
    "           table = self.target_table\n",
    "\n",
    "       return self.cur.execute(\n",
    "           f\"SELECT COUNT(*) FROM {self.schema}.{table};\"\n",
    "       ).fetchone()[0]\n",
    "\n",
    "   def get_max(self, column, table=None) -> int:\n",
    "\n",
    "       if table is None:\n",
    "           table = self.target_table\n",
    "\n",
    "       return self.cur.execute(\n",
    "           f\"SELECT MAX({column}) FROM {self.schema}.{table};\"\n",
    "       ).fetchone()[0]\n",
    "\n",
    "   def merge_table(self, cols, unique_columns):\n",
    "\n",
    "       on = \",\".join([f'TARGET.\"{col}\" = SOURCE.\"{col}\"' for col in unique_columns])\n",
    "       update = \",\".join([f'TARGET.\"{col}\" = SOURCE.\"{col}\"' for col in cols])\n",
    "       columns = \",\".join([f'\"{col}\"' for col in cols])\n",
    "       values = \",\".join([f'SOURCE.\"{col}\"' for col in cols])\n",
    "\n",
    "       stage_statement = \"\"\n",
    "       for col in cols:\n",
    "           if col == \"DWH_TIMESTAMP\":\n",
    "               stage_statement += (\n",
    "                   f'to_timestamp($1:\"DWH_TIMESTAMP\"::int, 9) AS \"DWH_TIMESTAMP\",'\n",
    "               )\n",
    "           else:\n",
    "               stage_statement += f'$1:\"{col}\" AS \"{col}\",'\n",
    "\n",
    "       stage_statement = f\"SELECT {stage_statement[:-1]} FROM @{self.target_stg_table}\"\n",
    "\n",
    "       sql_statement = f\"\"\"\n",
    "       MERGE INTO\n",
    "           {self.schema}.{self.target_table} AS TARGET\n",
    "       USING\n",
    "           ({stage_statement}) AS SOURCE\n",
    "       ON {on}\n",
    "       WHEN MATCHED THEN\n",
    "       UPDATE SET {update}\n",
    "       WHEN NOT MATCHED THEN\n",
    "       INSERT ({columns})\n",
    "       VALUES ({values})\n",
    "       \"\"\"\n",
    "\n",
    "       return self.cur.execute(sql_statement).fetchone()[0]\n",
    "\n",
    "ETLMode = {\"full_refresh\": \"full_refresh\", \"incremental\": \"incremental\"}\n",
    "ETLMode = namedtuple(\"x\", ETLMode.keys())(*ETLMode.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "db4a9843",
   "metadata": {},
   "outputs": [],
   "source": [
    "def source_to_parquet(\n",
    "   tablename,\n",
    "   filename,\n",
    "   db_conn,\n",
    "   etl_mode,\n",
    "   path,\n",
    "   sheet=None,\n",
    "   skiprows=0,\n",
    "   excel=False,\n",
    "   offset=0,\n",
    "):\n",
    "   _file = os.path.join(path, filename)\n",
    "\n",
    "   if excel == True:\n",
    "       df = get_excel_df(_file,sheet,skiprows)       \n",
    "   else:\n",
    "       df = pd.read_csv(_file, skiprows=skiprows, dtype=str)\n",
    "\n",
    "   if etl_mode != ETLMode.full_refresh:\n",
    "       if df.shape[0] + offset < db_conn.get_count():\n",
    "           raise Exception(\n",
    "               f\"DWH table:{tablename} has more count than source table:{filename}\"\n",
    "           )\n",
    "\n",
    "   df.columns = [col.upper() for col in df.columns]\n",
    "   df.insert(0, \"ID\", df.index.values + offset + 2 + skiprows)\n",
    "   df[\"DWH_TIMESTAMP\"] = datetime.now().replace(microsecond=0)\n",
    "\n",
    "   df.dropna(how=\"all\")\n",
    "\n",
    "   df.to_parquet(\n",
    "       path + f\"{tablename}_parquet.gzip\",\n",
    "       compression=\"gzip\",\n",
    "       engine=\"fastparquet\",\n",
    "       index=False,\n",
    "   )\n",
    "\n",
    "   return get_col_def(df.columns), df.columns\n",
    "\n",
    "\n",
    "\n",
    "def df_to_parquet(df, tablename, path = report_folder__cdm, offset = 0, skiprows = 0):\n",
    "\n",
    "   df.columns = [col.upper() for col in df.columns]\n",
    "   df.insert(0, \"ID\", df.index.values + offset + 2 + skiprows)\n",
    "   df[\"DWH_TIMESTAMP\"] = datetime.now().replace(microsecond=0)\n",
    "\n",
    "   df.dropna(how=\"all\")\n",
    "\n",
    "   df.to_parquet(\n",
    "       path + f\"{tablename}_parquet.gzip\",\n",
    "       compression=\"gzip\",\n",
    "       engine=\"fastparquet\",\n",
    "       index=False,\n",
    "   )\n",
    "\n",
    "   return get_col_def(df.columns), df.columns\n",
    "\n",
    "\n",
    "\n",
    "def get_col_def(cols):\n",
    "\n",
    "   col_definition = \"\"\n",
    "   for col in cols:\n",
    "       if col == \"DWH_TIMESTAMP\":\n",
    "           col_definition += f'\"{col}\" TIMESTAMP,'\n",
    "       elif col == \"ID\":\n",
    "           col_definition += f'\"{col}\" INT,'\n",
    "       else:\n",
    "           col_definition += f'\"{col}\" VARCHAR,'\n",
    "\n",
    "   return col_definition[:-1]\n",
    "\n",
    "\n",
    "\n",
    "def get_excel_df(_file,sheet,skiprows):\n",
    "  \n",
    "   wb = load_workbook(_file,read_only=True,data_only=True,keep_links=False)\n",
    "   ws = wb[sheet]\n",
    "   data = ws.values\n",
    "\n",
    "   for _ in range(skiprows):\n",
    "       next(data)\n",
    "\n",
    "   cols = next(data)\n",
    "\n",
    "   counter = dict(Counter(cols).items())\n",
    "   for _key, _value in counter.items():\n",
    "       counter[_key] = {'encounter':0,'cnt':_value}\n",
    "\n",
    "   new_cols = []\n",
    "\n",
    "   for col in cols:\n",
    "       if counter[col]['encounter'] > 0:\n",
    "           new_cols.append(f\"{col}.{counter[col]['encounter']}\")\n",
    "       else:\n",
    "           new_cols.append(col)\n",
    "\n",
    "       counter[col]['encounter'] += 1\n",
    "\n",
    "   df = pd.DataFrame(data,dtype=str,columns=new_cols)\n",
    "   wb.close()\n",
    "   df = df.loc[:,df.columns.values != None]\n",
    "   df = df.loc[:,~df.columns.str.contains(\"None\")]\n",
    "\n",
    "   return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ee1a95ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_upload__to_snowflake(schema, tablename, path, filename, mode):\n",
    "  \n",
    "   sf_loader = SnowflakeLoader(schema, tablename)\n",
    "   print(\"====Connected to snowflake====\")\n",
    "\n",
    "   col_def, cols = source_to_parquet(\n",
    "       tablename = tablename,\n",
    "       filename = filename,\n",
    "       db_conn = sf_loader,\n",
    "       etl_mode = ETLMode.full_refresh, #incremental #full_refresh\n",
    "       sheet=None,\n",
    "       path= path,\n",
    "       skiprows=0,\n",
    "       excel=False,\n",
    "       offset=0,\n",
    "   )\n",
    "\n",
    "   print(f\"====Converted csv/excel {filename} to parquet====\")\n",
    "\n",
    "   try:\n",
    "       print(1)\n",
    "       sf_loader.create_table(col_def)\n",
    "       print(2)\n",
    "       sf_loader.create_stg_table()\n",
    "       print(3)\n",
    "       sf_loader.put_file(os.path.join(path, f\"{tablename}_parquet.gzip\"))\n",
    "       print(4)\n",
    "\n",
    "       if mode == ETLMode.full_refresh:\n",
    "           print(5)\n",
    "           sf_loader.overwrite_target_table(col_def, cols)\n",
    "       elif mode == ETLMode.incremental:\n",
    "           print(6)\n",
    "           sf_loader.merge_table(cols, unique_key.split(\",\"))\n",
    "       print(f\"====Loaded the {tablename} to DWH====\")\n",
    "   except Exception as e:\n",
    "       logging.exception(e)\n",
    "   finally:\n",
    "       os.remove(path + f\"{tablename}_parquet.gzip\")\n",
    "       sf_loader.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "50e64663",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:snowflake.connector.connection:Snowflake Connector for Python Version: 2.7.12, Python Version: 3.9.13, Platform: Windows-10-10.0.19044-SP0\n",
      "INFO:snowflake.connector.connection:This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ingest table cdm__quarterly_data to snowflake\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:snowflake.connector.cursor:query: [CREATE TABLE IF NOT EXISTS CDM__QUARTERLY_DATA (\"ID\" INT,\"YEAR\" VARCHAR,\"WEEK\" V...]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Connected to snowflake====\n",
      "====Converted csv/excel cdm__quarterly_data.csv to parquet====\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:snowflake.connector.cursor:query execution done\n",
      "INFO:snowflake.connector.cursor:query: [CREATE OR REPLACE STAGE cdm__quarterly_data_STG file_format = (type = 'PARQUET')...]\n",
      "INFO:snowflake.connector.cursor:query execution done\n",
      "INFO:snowflake.connector.cursor:query: [put file://c:/cdm/cdm__quarterly_data_parquet.gzip @cdm__quarterly_data_STG;]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:snowflake.connector.cursor:query execution done\n",
      "INFO:snowflake.connector.cursor:query: [DROP TABLE IF EXISTS swap_CDM__QUARTERLY_DATA]\n",
      "INFO:snowflake.connector.cursor:query execution done\n",
      "INFO:snowflake.connector.cursor:query: [CREATE TABLE IF NOT EXISTS swap_CDM__QUARTERLY_DATA (\"ID\" INT,\"YEAR\" VARCHAR,\"WE...]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:snowflake.connector.cursor:query execution done\n",
      "INFO:snowflake.connector.cursor:query: [COPY INTO swap_CDM__QUARTERLY_DATA FROM (SELECT $1:\"ID\",$1:\"YEAR\",$1:\"WEEK\",$1:\"...]\n",
      "INFO:snowflake.connector.cursor:query execution done\n",
      "INFO:snowflake.connector.cursor:query: [DROP TABLE IF EXISTS temp_CDM__QUARTERLY_DATA]\n",
      "INFO:snowflake.connector.cursor:query execution done\n",
      "INFO:snowflake.connector.cursor:query: [ALTER TABLE CDM__QUARTERLY_DATA RENAME TO temp_CDM__QUARTERLY_DATA;]\n",
      "INFO:snowflake.connector.cursor:query execution done\n",
      "INFO:snowflake.connector.cursor:query: [ALTER TABLE swap_CDM__QUARTERLY_DATA RENAME TO CDM__QUARTERLY_DATA;]\n",
      "INFO:snowflake.connector.cursor:query execution done\n",
      "INFO:snowflake.connector.connection:closed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Loaded the cdm__quarterly_data to DWH====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:snowflake.connector.connection:No async queries seem to be running, deleting session\n",
      "INFO:snowflake.connector.connection:Snowflake Connector for Python Version: 2.7.12, Python Version: 3.9.13, Platform: Windows-10-10.0.19044-SP0\n",
      "INFO:snowflake.connector.connection:This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ingest table cdm__seller_code_mapping to snowflake\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:snowflake.connector.cursor:query: [CREATE TABLE IF NOT EXISTS CDM__SELLER_CODE_MAPPING (\"ID\" INT,\"SELLER SHORT CODE...]\n",
      "INFO:snowflake.connector.cursor:query execution done\n",
      "INFO:snowflake.connector.cursor:query: [CREATE OR REPLACE STAGE cdm__seller_code_mapping_STG file_format = (type = 'PARQ...]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Connected to snowflake====\n",
      "====Converted csv/excel cdm__seller_code_mapping.csv to parquet====\n",
      "1\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:snowflake.connector.cursor:query execution done\n",
      "INFO:snowflake.connector.cursor:query: [put file://c:/cdm/cdm__seller_code_mapping_parquet.gzip @cdm__seller_code_mappin...]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:snowflake.connector.cursor:query execution done\n",
      "INFO:snowflake.connector.cursor:query: [DROP TABLE IF EXISTS swap_CDM__SELLER_CODE_MAPPING]\n",
      "INFO:snowflake.connector.cursor:query execution done\n",
      "INFO:snowflake.connector.cursor:query: [CREATE TABLE IF NOT EXISTS swap_CDM__SELLER_CODE_MAPPING (\"ID\" INT,\"SELLER SHORT...]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:snowflake.connector.cursor:query execution done\n",
      "INFO:snowflake.connector.cursor:query: [COPY INTO swap_CDM__SELLER_CODE_MAPPING FROM (SELECT $1:\"ID\",$1:\"SELLER SHORT CO...]\n",
      "INFO:snowflake.connector.cursor:query execution done\n",
      "INFO:snowflake.connector.cursor:query: [DROP TABLE IF EXISTS temp_CDM__SELLER_CODE_MAPPING]\n",
      "INFO:snowflake.connector.cursor:query execution done\n",
      "INFO:snowflake.connector.cursor:query: [ALTER TABLE CDM__SELLER_CODE_MAPPING RENAME TO temp_CDM__SELLER_CODE_MAPPING;]\n",
      "INFO:snowflake.connector.cursor:query execution done\n",
      "INFO:snowflake.connector.cursor:query: [ALTER TABLE swap_CDM__SELLER_CODE_MAPPING RENAME TO CDM__SELLER_CODE_MAPPING;]\n",
      "INFO:snowflake.connector.cursor:query execution done\n",
      "INFO:snowflake.connector.connection:closed\n",
      "INFO:snowflake.connector.connection:No async queries seem to be running, deleting session\n",
      "INFO:snowflake.connector.connection:Snowflake Connector for Python Version: 2.7.12, Python Version: 3.9.13, Platform: Windows-10-10.0.19044-SP0\n",
      "INFO:snowflake.connector.connection:This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Loaded the cdm__seller_code_mapping to DWH====\n",
      "\n",
      "Ingest table cdm__weekly_data to snowflake\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:snowflake.connector.cursor:query: [CREATE TABLE IF NOT EXISTS CDM__WEEKLY_DATA (\"ID\" INT,\"YEAR\" VARCHAR,\"WEEK\" VARC...]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Connected to snowflake====\n",
      "====Converted csv/excel cdm__weekly_data.csv to parquet====\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:snowflake.connector.cursor:query execution done\n",
      "INFO:snowflake.connector.cursor:query: [CREATE OR REPLACE STAGE cdm__weekly_data_STG file_format = (type = 'PARQUET');]\n",
      "INFO:snowflake.connector.cursor:query execution done\n",
      "INFO:snowflake.connector.cursor:query: [put file://c:/cdm/cdm__weekly_data_parquet.gzip @cdm__weekly_data_STG;]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:snowflake.connector.cursor:query execution done\n",
      "INFO:snowflake.connector.cursor:query: [DROP TABLE IF EXISTS swap_CDM__WEEKLY_DATA]\n",
      "INFO:snowflake.connector.cursor:query execution done\n",
      "INFO:snowflake.connector.cursor:query: [CREATE TABLE IF NOT EXISTS swap_CDM__WEEKLY_DATA (\"ID\" INT,\"YEAR\" VARCHAR,\"WEEK\"...]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:snowflake.connector.cursor:query execution done\n",
      "INFO:snowflake.connector.cursor:query: [COPY INTO swap_CDM__WEEKLY_DATA FROM (SELECT $1:\"ID\",$1:\"YEAR\",$1:\"WEEK\",$1:\"COU...]\n",
      "INFO:snowflake.connector.cursor:query execution done\n",
      "INFO:snowflake.connector.cursor:query: [DROP TABLE IF EXISTS temp_CDM__WEEKLY_DATA]\n",
      "INFO:snowflake.connector.cursor:query execution done\n",
      "INFO:snowflake.connector.cursor:query: [ALTER TABLE CDM__WEEKLY_DATA RENAME TO temp_CDM__WEEKLY_DATA;]\n",
      "INFO:snowflake.connector.cursor:query execution done\n",
      "INFO:snowflake.connector.cursor:query: [ALTER TABLE swap_CDM__WEEKLY_DATA RENAME TO CDM__WEEKLY_DATA;]\n",
      "INFO:snowflake.connector.cursor:query execution done\n",
      "INFO:snowflake.connector.connection:closed\n",
      "INFO:snowflake.connector.connection:No async queries seem to be running, deleting session\n",
      "INFO:snowflake.connector.connection:Snowflake Connector for Python Version: 2.7.12, Python Version: 3.9.13, Platform: Windows-10-10.0.19044-SP0\n",
      "INFO:snowflake.connector.connection:This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Loaded the cdm__weekly_data to DWH====\n",
      "\n",
      "Ingest table certificate_target to snowflake\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:snowflake.connector.cursor:query: [CREATE TABLE IF NOT EXISTS CERTIFICATE_TARGET (\"ID\" INT,\"START_DATE\" VARCHAR,\"EN...]\n",
      "INFO:snowflake.connector.cursor:query execution done\n",
      "INFO:snowflake.connector.cursor:query: [CREATE OR REPLACE STAGE certificate_target_STG file_format = (type = 'PARQUET');...]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Connected to snowflake====\n",
      "====Converted csv/excel certificate_target.csv to parquet====\n",
      "1\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:snowflake.connector.cursor:query execution done\n",
      "INFO:snowflake.connector.cursor:query: [put file://c:/cdm/certificate_target_parquet.gzip @certificate_target_STG;]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:snowflake.connector.cursor:query execution done\n",
      "INFO:snowflake.connector.cursor:query: [DROP TABLE IF EXISTS swap_CERTIFICATE_TARGET]\n",
      "INFO:snowflake.connector.cursor:query execution done\n",
      "INFO:snowflake.connector.cursor:query: [CREATE TABLE IF NOT EXISTS swap_CERTIFICATE_TARGET (\"ID\" INT,\"START_DATE\" VARCHA...]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:snowflake.connector.cursor:query execution done\n",
      "INFO:snowflake.connector.cursor:query: [COPY INTO swap_CERTIFICATE_TARGET FROM (SELECT $1:\"ID\",$1:\"START_DATE\",$1:\"END_D...]\n",
      "INFO:snowflake.connector.cursor:query execution done\n",
      "INFO:snowflake.connector.cursor:query: [DROP TABLE IF EXISTS temp_CERTIFICATE_TARGET]\n",
      "INFO:snowflake.connector.cursor:query execution done\n",
      "INFO:snowflake.connector.cursor:query: [ALTER TABLE CERTIFICATE_TARGET RENAME TO temp_CERTIFICATE_TARGET;]\n",
      "INFO:snowflake.connector.cursor:query execution done\n",
      "INFO:snowflake.connector.cursor:query: [ALTER TABLE swap_CERTIFICATE_TARGET RENAME TO CERTIFICATE_TARGET;]\n",
      "INFO:snowflake.connector.cursor:query execution done\n",
      "INFO:snowflake.connector.connection:closed\n",
      "INFO:snowflake.connector.connection:No async queries seem to be running, deleting session\n",
      "INFO:snowflake.connector.connection:Snowflake Connector for Python Version: 2.7.12, Python Version: 3.9.13, Platform: Windows-10-10.0.19044-SP0\n",
      "INFO:snowflake.connector.connection:This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Loaded the certificate_target to DWH====\n",
      "\n",
      "Ingest table point_target to snowflake\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:snowflake.connector.cursor:query: [CREATE TABLE IF NOT EXISTS POINT_TARGET (\"ID\" INT,\"YEAR\" VARCHAR,\"QUARTER\" VARCH...]\n",
      "INFO:snowflake.connector.cursor:query execution done\n",
      "INFO:snowflake.connector.cursor:query: [CREATE OR REPLACE STAGE point_target_STG file_format = (type = 'PARQUET');]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Connected to snowflake====\n",
      "====Converted csv/excel point_target.csv to parquet====\n",
      "1\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:snowflake.connector.cursor:query execution done\n",
      "INFO:snowflake.connector.cursor:query: [put file://c:/cdm/point_target_parquet.gzip @point_target_STG;]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:snowflake.connector.cursor:query execution done\n",
      "INFO:snowflake.connector.cursor:query: [DROP TABLE IF EXISTS swap_POINT_TARGET]\n",
      "INFO:snowflake.connector.cursor:query execution done\n",
      "INFO:snowflake.connector.cursor:query: [CREATE TABLE IF NOT EXISTS swap_POINT_TARGET (\"ID\" INT,\"YEAR\" VARCHAR,\"QUARTER\" ...]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:snowflake.connector.cursor:query execution done\n",
      "INFO:snowflake.connector.cursor:query: [COPY INTO swap_POINT_TARGET FROM (SELECT $1:\"ID\",$1:\"YEAR\",$1:\"QUARTER\",$1:\"REGI...]\n",
      "INFO:snowflake.connector.cursor:query execution done\n",
      "INFO:snowflake.connector.cursor:query: [DROP TABLE IF EXISTS temp_POINT_TARGET]\n",
      "INFO:snowflake.connector.cursor:query execution done\n",
      "INFO:snowflake.connector.cursor:query: [ALTER TABLE POINT_TARGET RENAME TO temp_POINT_TARGET;]\n",
      "INFO:snowflake.connector.cursor:query execution done\n",
      "INFO:snowflake.connector.cursor:query: [ALTER TABLE swap_POINT_TARGET RENAME TO POINT_TARGET;]\n",
      "INFO:snowflake.connector.cursor:query execution done\n",
      "INFO:snowflake.connector.connection:closed\n",
      "INFO:snowflake.connector.connection:No async queries seem to be running, deleting session\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Loaded the point_target to DWH====\n"
     ]
    }
   ],
   "source": [
    "for file_path in raw_lists:\n",
    "    file_name = file_path.split(\"/\")[-1] \n",
    "    table_name = file_name.split('.')[0]\n",
    "    print(f'\\nIngest table {table_name} to snowflake')\n",
    "\n",
    "    csv_upload__to_snowflake(\n",
    "        schema = 'CDM_DATA'\n",
    "    , tablename = table_name\n",
    "    , path = report_folder__cdm\n",
    "    , filename = file_name\n",
    "    , mode = ETLMode.full_refresh\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "11938c6bc6919ae2720b4d5011047913343b08a43b18698fd82dedb0d4417594"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
